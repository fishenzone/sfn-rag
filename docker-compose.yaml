services:
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant_db
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant_data:/qdrant/storage
    restart: always

  # vllm:
  #   image: vllm/vllm-openai:latest
  #   container_name: vllm_server
  #   ports:
  #     - "8000:8000"
  #   volumes:
  #     - ~/.cache/huggingface:/root/.cache/huggingface
  #   command: [
  #       "--model", "Qwen/Qwen2.5-7B-Instruct",
  #       "--max-model-len", "32768",
  #       "--gpu-memory-utilization", "0.6",
  #       "--host", "0.0.0.0",
  #       "--port", "8000"
  #     ]
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   restart: always
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11435:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: always

  app:
    container_name: rag_app
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    volumes:
      - ./app:/app/app
      - ./chat_histories:/app/chat_histories
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8001", "--reload"]
    depends_on:
      - qdrant
      # - vllm
      - ollama

volumes:
  ollama_data:
