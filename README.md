# SFN AI Chat Bot (RAG)

## Описание Проекта

Проект представляет собой простого ИИ чат-бота на базе Retrieval-Augmented Generation (RAG), разработанного для ООО «СФН». Чат-бот предназначен для предоставления точных и полезных ответов на вопросы инвесторов, используя внутреннюю базу знаний, собранную с сайта компании и публичных источников.

## Требования Задания

* Реализация RAG-пайплайна на Python.
* Использование LLM (в данной реализации используется Ollama).
* Использование векторного хранилища (в данной реализации используется Qdrant).
* Подготовка базы знаний (сбор данных, чанкинг, эмбеддинги, сохранение).
* Реализация взаимодействия через API (FastAPI).
* Базовая документация (README с инструкциями).
* Верхнеуровневая архитектура RAG-пайплайна и рекомендации по улучшению.
* Обработка ошибок (базовая).
* Базовое логирование.

## Структура Проекта
.
├── app/
│   ├── data/
│   │   └── sfn_data.txt           # База знаний (пример)
│   ├── services/
│   │   ├── chat_engine.py         # Логика чата
│   │   ├── document_processor.py  # Обработка документов
│   │   └── vector_store.py        # Взаимодействие с векторным хранилищем
│   ├── static/
│   │   ├── index.html             # Интерфейс чата
│   │   └── history_viewer.html    # Интерфейс истории чатов
│   ├── utils/
│   │   └── llm_client.py          # Взаимодействие с LLM
│   ├── init.py
    ├── config.py                  # Переменные конфигурации
│   └── main.py                    # Основное FastAPI приложение
├── chat_histories/                # Директория для сохранения истории чатов (создается при запуске)
├── qdrant_data/                   # Директория для персистентного хранения данных Qdrant (создается при запуске)
├── Dockerfile                     # Определение Docker образа
├── docker-compose.yaml            # Определение сервисов Docker Compose
├── requirements.txt               # Зависимости Python
└── setup.sh                       # Скрипт начальной настройки

## Запуск Проекта

Проект использует Docker и Docker Compose для упрощения развертывания.

1.  **Клонируйте репозиторий:**
    ```bash
    git clone git@github.com:fishenzone/sfn-rag.git
    cd sfn-rag
    ```

2.  **Выполните скрипт начальной настройки:**
    Этот скрипт устанавливает Python зависимости и загружает необходимую embedding модель.
    *(Примечание: Часть скрипта, связанная с загрузкой модели для vLLM, является лишней в текущей конфигурации с Ollama, но оставлена как есть)*

    ```bash
    chmod +x setup.sh
    ./setup.sh
    ```

3.  **Загрузите модель для Ollama:**
    Перед запуском контейнеров необходимо убедиться, что в сервисе Ollama доступна модель, указанная в `config.py` (`qwen3:30b-a3b` по умолчанию). Выполните следующую команду:

    ```bash
    docker exec -it ollama ollama pull qwen3:30b-a3b
    ```
    *(Убедитесь, что контейнер Ollama запущен, прежде чем выполнять эту команду. Возможно, потребуется запустить `docker-compose up -d ollama` сначала, до полного `docker-compose up`)*

4.  **Соберите и запустите Docker контейнеры:**

    ```bash
    docker-compose up --build
    ```

После успешного запуска сервисов:

* Интерфейс чат-бота будет доступен по адресу: `http://localhost:8001/`
* Интерфейс просмотра истории чатов будет доступен по адресу: `http://localhost:8001/history`
* API документация (Swagger UI) доступна по адресу: `http://localhost:8001/docs`

**Развернутое Демо:**

Демонстрация проекта также доступна по следующим публичным адресам:

* Чат-бот: `http://172.208.114.221:8001/`
* История чатов: `http://172.208.114.221:8001/history`

## Архитектура RAG-пайплайна

Верхнеуровневая архитектура RAG-пайплайна выглядит следующим образом:

1.  **Индексация Данных:**
    * Загрузка текстовых данных из файлов (`app/data/sfn_data.txt`).
    * Разбиение текста на перекрывающиеся чанки (с использованием `RecursiveCharacterTextSplitter`).
    * Удаление дублирующихся чанков для повышения эффективности (предпринята попытка).
    * Генерация векторных эмбеддингов для каждого чанка с использованием предобученной модели (`sergeyzh/BERTA`).
    * Сохранение чанков и их эмбеддингов в векторном хранилище Qdrant в отдельной коллекции (`sfn_knowledge`).

2.  **Обработка Запроса Пользователя:**
    * Пользователь отправляет текстовый запрос через веб-интерфейс или напрямую к API `/api/chat`.
    * Генерация векторного эмбеддинга для запроса пользователя.
    * Поиск по векторному хранилищу Qdrant для нахождения наиболее релевантных чанков из базы знаний (топ-5 по умолчанию) на основе сходства эмбеддингов.
    * Формирование промпта для LLM, включающего оригинальный запрос пользователя и текст найденных релевантных чанков (контекст).
    * Отправка сформированного промпта в LLM (Ollama) для генерации ответа.
    * Получение ответа от LLM.
    * Возврат сгенерированного ответа пользователю, включая список источников (обрезанный текст релевантных чанков).
    * Сохранение диалога (запрос пользователя и ответ бота) в истории чата (в памяти и асинхронно в JSON файл).

## Обоснование Выбора Qdrant вместо FAISS

Вместо FAISS, указанного в задании, было выбрано векторное хранилище Qdrant по следующим причинам:

* **Персистентность:** Qdrant может сохранять данные на диске (`./qdrant_data`), что предотвращает потерю индекса при перезапуске приложения, в отличие от FAISS, который обычно хранит индекс в оперативной памяти.
* **Производственная Готовность:** Qdrant разработан как полнофункциональное векторное СУБД с возможностями шардинга, распределения данных и эффективной работы с большими объемами данных, что делает его более подходящим для реальных проектов по сравнению с более статичным FAISS.
* **Гибкость:** Qdrant позволяет легко обновлять или удалять данные в коллекции, тогда как FAISS требует полной переиндексации при изменении базы знаний.
* **Эффективность ввода/вывода:** Qdrant оптимизирован для считывания данных с диска по частям, не требуя полной загрузки всего индекса в RAM, что важно для больших наборов данных.

## Реализованные Функции

* Обработка документов (.txt, .pdf) и создание векторного индекса.
* Поиск релевантных документов с использованием векторного сходства.
* Генерация ответов LLM с учетом найденного контекста (RAG).
* Простой веб-интерфейс для взаимодействия (FastAPI + HTML/JS).
* Управление сессиями чата и сохранение истории.
* Базовое логирование запросов и ответов.
* Health check endpoint (`/health`).

## Рекомендации по Улучшению

1.  **Улучшение Качества Базы Знаний:**
    * **Более Качественный Сбор Данных:** Использовать более точный парсинг сайта (например, с учетом структуры HTML) или API, если доступно, чтобы извлекать более структурированную информацию.
    * **Обогащение Метаданными:** Сохранять вместе с текстовыми чанками метаданные, такие как URL источника, название раздела, дата обновления информации. Это позволит отображать более полезные источники в ответе (например, с активной гиперссылкой на страницу).
    * **Обработка Дубликатов:** Применить более строгие методы определения и удаления дубликатов или очень похожих чанков.

2.  **Оптимизация Индексации и Поиска:**
    * **Продвинутый Чанкинг:** Использовать более умные стратегии разбиения на чанки, учитывающие структуру документа (параграфы, разделы). Можно экспериментировать с разными размерами чанков или даже создать несколько индексов с чанками разного размера.
    * **Реранкер:** Добавить этап реранкинга после получения первичных результатов из векторного хранилища. Реранкер (например, на основе более мощной кросс-энкодерной модели) может переупорядочить найденные чанки, поместив наиболее релевантные на верхние позиции для LLM.

3.  **Улучшение RAG-Пайплайна:**
    * **Выбор Чанков:** Реализовать более сложные стратегии выбора чанков для включения в промпт, возможно, учитывая историю диалога или уверенность в релевантности найденных чанков.
    * **Формирование Промпта:** Экспериментировать с различными структурами промпта и системными сообщениями для LLM для получения более точных и связных ответов.

4.  **Расширение Функциональности:**
    * **Персистентное Хранение Истории Чатов:** Перенести хранение истории чатов из JSON файлов в полноценную базу данных (SQL или NoSQL) для лучшей масштабируемости, надежности и удобства анализа.
    * **Классификация Запросов/Агентский Подход:** Если появится необходимость работать с несколькими разнородными базами знаний (например, "финансы" и "недвижимость"), реализовать модуль классификации запросов. Этот модуль будет определять тему запроса и направлять его в соответствующую базу знаний или к специализированному "агенту", который работает с конкретной областью данных.
    * **Мониторинг и Аналитика:** Внедрить более продвинутые системы мониторинга (например, Prometheus, Grafana) для отслеживания производительности API, LLM, векторного хранилища. Собирать метрики по качеству ответов, длительности обработки запросов.

5.  **Пользовательский Интерфейс:**
    * Улучшить внешний вид и интерактивность веб-интерфейса.
    * Реализовать отображение источников в более удобном формате (например, модальное окно с полным текстом или ссылкой).

## Текстовое Описание Архитектуры

Данное README включает текстовое описание верхнеуровневой архитектуры RAG-пайплайна в разделе "Архитектура RAG-пайплайна".